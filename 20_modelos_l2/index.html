<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>SPAN 585</title>
    <meta charset="utf-8" />
    <meta name="author" content="Joseph V. Casillas, PhD" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link href="libs/remark-css/hygge.css" rel="stylesheet" />
    <link href="libs/remark-css/rutgers.css" rel="stylesheet" />
    <link href="libs/remark-css/rutgers-fonts.css" rel="stylesheet" />
    <link href="libs/tile-view/tile-view.css" rel="stylesheet" />
    <script src="libs/tile-view/tile-view.js"></script>
    <link href="libs/animate.css/animate.xaringan.css" rel="stylesheet" />
    <link href="libs/tachyons/tachyons.min.css" rel="stylesheet" />
    <link href="libs/panelset/panelset.css" rel="stylesheet" />
    <script src="libs/panelset/panelset.js"></script>
    <script type="application/json" id="xaringanExtra-editable-docid">{"id":"xc5f0a41c0134d8a839500aea8dd14c4","expires":14}</script>
    <script src="libs/himalaya/himalaya.js"></script>
    <script src="libs/js-cookie/js.cookie.js"></script>
    <link href="libs/editable/editable.css" rel="stylesheet" />
    <script src="libs/editable/editable.js"></script>
    <script src="libs/xaringanExtra-webcam/webcam.js"></script>
    <script id="xaringanExtra-webcam-options" type="application/json">{"width":"200","height":"200","margin":"1em"}</script>
    <script src="https://use.fontawesome.com/5235085b15.js"></script>
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# SPAN 585
]
.subtitle[
## La fonética/fonología del bilingüismo
]
.author[
### Joseph V. Casillas, PhD
]
.date[
### Rutgers University | Spring 2024</br>Last update: 01/05/24
]

---






background-image: url(./libs/img/niko0a.png)
background-size: contain
background-color: black

---
background-image: url(./libs/img/niko0b.png)
background-size: contain
background-color: black

---
background-image: url(./libs/img/niko1a.png)
background-size: contain
background-color: black

---
background-image: url(./libs/img/niko1b.png)
background-size: contain
background-color: black

---
background-image: url(./libs/img/niko2a.png)
background-size: contain
background-color: black

---
background-image: url(./libs/img/niko2b.png)
background-size: contain
background-color: black

---
background-image: url(./libs/img/niko3a.png)
background-size: contain
background-color: black

---
background-image: url(./libs/img/niko3b.png)
background-size: contain
background-color: black

---
background-image: url(./libs/img/niko4a.png)
background-size: contain
background-color: black

---
background-image: url(./libs/img/niko4b.png)
background-size: contain
background-color: black

---
background-image: url(./libs/img/niko5a.png)
background-size: contain
background-color: black

---
background-image: url(./libs/img/niko5b.png)
background-size: contain
background-color: black

---
background-image: url(./libs/img/distributions.png)
background-size: 1000px
background-color: #F3F4DF

---
background-image: url(./libs/img/sheep.jpg), url(./libs/img/ship.jpg)
background-size: 500px, 500px
background-color: black
background-position: 5% 50%, 95% 50%

---
background-image: url(./libs/img/timeline.png)
background-size: contain
background-color: black

.footnote[.grey[Kuhl, 2004]]

---

# Models

### SLM-r

### PAM, PAM-L2

### L2LP

### Others

---

# Models

### Speech Learning Model (SLM)

- Ability to learn L2 sounds remains throughout lifespan

- L1 and L2 sounds share the same phonetic space, therefore they interact with 
each other

- L2 phonetic categories are stored in long-term memory

- Bidirectional influence L1 ︎←→ L2

- Equivalence classification: L2 phones perceived as being acoustically 
similar to native categories are difficult to perceive (learn, produce)

- Sounds that are sufficiently different are easily perceived, new phonetic 
categories are formed

.footnote[Flege (1995)]

---

# Models

### Perceptual Assimilation Model (PAM, PAM-L2)

- Initial iteration was concerned with naïve listeners’ perceptual 
assimilation of non-native segments
- Was later extended to encompass L2 learners
- Gestural model (DRT)
- Cross-linguistic interference explained by erroneously perceptually 
assimilating gestures of the speech signal to those of L1 phonology (based on similarities/differences to native categories)

--

&lt;p&gt;&lt;/p&gt;

- 3 possibilities for perceptual assimilation of segments
  - Assimilated to native category: good, acceptable, or deviant to native 
  segment
  - Assimilated as uncategorizable speech sound (but in phonetic space)
  - Not assimilated as speech (non-speech sound, outside phonetic space)

--

&lt;p&gt;&lt;/p&gt;

- 6 possibilities for perceptual assimilation of contrasts

.footnote[Best (1995), Best &amp; Tyler (2007)]

---

# Models 

### Perceptual Assimilation Model (PAM, PAM-L2)

#### 6 possibilities for perceptual assimilation of contrasts

1. **two-category assimilation** (TC): non-native segments individually 
assimilated to distinct native categories (prediction: easy) 
2. **category goodness** (CG): two segments are assimilated to single native 
category, one notably more acceptable than the other (prediction: moderate to 
very good)
3. **single category assimilation** (SC): both non-native segments assimilated to 
single native category and considered equally good or bad exemplars 
(prediction: poor)

--
4. **uncategorizable** (UU): both non-native segments presumed to be part of 
native phonetic space but don't pertain to any native category (prediction: 
variable as function of proximity to neighboring categories and each other)
5. **uncategorized-categorized** (UC): only one of the segments is deemed 
uncategorizable (but both are presumed to be within the native phonetic space) (prediction: very good)
6. **nonassimilable** (NA): both non-native sounds outside spech domain 
(prediction: variable) 

---

# Models

### Second Language Linguistic Perception Model (L2LP)

- Perceptually driven, computational model dealing with contrasts (like PAM), 
focus on vowels

- Based in Optimality Theory

- New L2 perceptual grammar is created via *full copying*/*full access*, 
develops independently of the L1 perception grammar

&lt;p&gt;&lt;/p&gt;

- During initial stages of learning L2 phonological system is copy of L1 system
  - Categories change/adjust when necessary via *gradual learning algorithm*
  - Novel categories formed when L1 categories insufficient

&lt;p&gt;&lt;/p&gt;

- Three learning scenarios: 
  - Similar scenario (easy): reuse L1 categories, adjust boundaries for 
  phonetic differences via GLA
  - New scenario (hard): L1 categories cannot be reused, new phonetic 
  categories must be formed
  - Subset scenario (hard): L2 contrast is mapped to multiple L1 categories

.footnote[Escudero (2005, 2009)]

---
background-image: url(./libs/img/pam1.png)
background-size: contain

---
background-image: url(./libs/img/pam2.png)
background-size: contain

---
background-image: url(./libs/img/pam3.png)
background-size: contain

---
background-image: url(./libs/img/pam4.png)
background-size: contain

---
background-image: url(./libs/img/pam5.png)
background-size: contain

---
background-image: url(./libs/img/pam6.png)
background-size: contain

---

# Models

### Compare and contrast

- All 3 models are perceptually driven

- SLM deals with segments, whereas PAM/PAM-L2 and L2LP deal with contrasts

- SLM and L2LP maintain learning possible at all ages

- SLM posits bidirectional influence (L1 ←︎→ L2)

- L2LP maintains phonological systems are separate, can be selectively 
activated

- L2LP proposes boundary resetting via error-driven learning algorithm, novel 
categories form when L1 categories cannot be reused

- Under the SLM phonetic category formation occurs when novel representations 
are stored in long-term memory... does not fully explain how exactly L2 
perception becomes automatized as in L1 perception

- Not entirely clear, nor is it unanimously agreed upon, as to what constitutes similarity, or equivalence

---

# Models

### Other models

#### Automatic Selective Perception (ASP, Strange, 2011) 

- Posits that Selective Perceptual Routines (SPRs) are responsible for 
automatically perceiving language-specific patterns. 
- Crucially, SPRs "[...] selectively detect phonetically-relevant 
differences in the stimuli while ignoring other acoustic variations which 
constitute within-L1-category variability" (Strange &amp; Shafer, 2008, p. 184). 
- Similar to the SLM, the ASP model maintains that SPRs can still be 
acquired and improved in L2 learners, although initially L1 SPRs are used to 
perceive L2 segments

---

# Models

### Other models

#### Native Language Magnet model (NLM, Kuhl 1992, 2000)

- Accounts for the transition from auditory to language-specific perceptual 
processing
- Three phases in development 
  - Phase 1 (initial state): infants capable of differentiating all sounds of 
  human speech, based on general auditory processing mechanisms 
  - Phase 2: infants' sensitivity to the distributional properties of 
  linguistic input produces phonetic representations based on the 
  distributional 'modes' in ambient speech input, experience 'warps' 
  perception, distortion decreases perceptual sensitivity near category modes 
  and increases perceptual sensitivity near the boundaries between categories. 
  As experience accumulates, representations most often activated (prototypes) 
  begin to function as perceptual magnets for other members of the category, 
  increasing the perceived similarity between members of the category 
  - Phase 3: distortion of perception, 'perceptual magnet effect', produces 
  facilitation in native and a reduction in foreign language phonetic abilities
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://cdnjs.cloudflare.com/ajax/libs/remark/0.14.0/remark.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "monokai",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
